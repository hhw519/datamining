{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "myfirst.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1De7AkKmnt6p6DfTu3fLViw-tyQMrO6Mg",
      "authorship_tag": "ABX9TyOoDA4ebceO16ZtfA/IPwTf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hhw519/datamining/blob/master/myfirst.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWPkDngyN31t",
        "colab_type": "code",
        "outputId": "61ace960-de51-4b82-b9fa-1620da70477f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "#%%\n",
        "import  os\n",
        "import  numpy as np\n",
        "from    tensorflow import keras\n",
        "from    tensorflow.keras import layers, losses, optimizers, Sequential\n",
        "\n",
        "#uploaded = files.upload()\n",
        "df = pd.read_csv('tran.csv',encoding='cp1252')\n",
        "target = df.pop('result')\n",
        "dataset = tf.data.Dataset.from_tensor_slices((df.values, target.values))\n",
        "dataset.batch(4)\n",
        "tf.random.set_seed(22)\n",
        "np.random.seed(22)\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "assert tf.__version__.startswith('2.')\n",
        "\n",
        "batchsz = 128 # 批量大小\n",
        "total_words = 10000 # 词汇表大小N_vocab\n",
        "max_review_len = 80 # 句子最大长度s，大于的句子部分将截断，小于的将填充\n",
        "embedding_len = 100 # 词向量特征长度f\n",
        "# 加载IMDB数据集，此处的数据采用数字编码，一个数字代表一个单词\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=total_words)\n",
        "print(x_train.shape, len(x_train[0]), y_train.shape)\n",
        "print(x_test.shape, len(x_test[0]), y_test.shape)\n",
        "#%%\n",
        "x_train[0]\n",
        "#%%\n",
        "# 数字编码表\n",
        "word_index = keras.datasets.imdb.get_word_index()\n",
        "# for k,v in word_index.items():\n",
        "#     print(k,v)\n",
        "#%%\n",
        "word_index = {k:(v+3) for k,v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2  # unknown\n",
        "word_index[\"<UNUSED>\"] = 3\n",
        "# 翻转编码表\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "decode_review(x_train[8])\n",
        "\n",
        "#%%\n",
        "\n",
        "# x_train:[b, 80]\n",
        "# x_test: [b, 80]\n",
        "# 截断和填充句子，使得等长，此处长句子保留句子后面的部分，短句子在前面填充\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_review_len)\n",
        "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_review_len)\n",
        "# 构建数据集，打散，批量，并丢掉最后一个不够batchsz的batch\n",
        "db_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "db_train = db_train.shuffle(1000).batch(batchsz, drop_remainder=True)\n",
        "db_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "db_test = db_test.batch(batchsz, drop_remainder=True)\n",
        "print('x_train shape:', x_train.shape, tf.reduce_max(y_train), tf.reduce_min(y_train))\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n",
        "#%%\n",
        "\n",
        "class MyRNN(keras.Model):\n",
        "    # Cell方式构建多层网络\n",
        "    def __init__(self, units):\n",
        "        super(MyRNN, self).__init__() \n",
        "        # 词向量编码 [b, 80] => [b, 80, 100]\n",
        "        self.embedding = layers.Embedding(total_words, embedding_len,\n",
        "                                          input_length=max_review_len)\n",
        "        # 构建RNN\n",
        "        self.rnn = keras.Sequential([\n",
        "            layers.GRU(units, dropout=0.5, return_sequences=True),\n",
        "            layers.GRU(units, dropout=0.5)\n",
        "        ])\n",
        "        # 构建分类网络，用于将CELL的输出特征进行分类，2分类\n",
        "        # [b, 80, 100] => [b, 64] => [b, 1]\n",
        "        self.outlayer = Sequential([\n",
        "        \tlayers.Dense(32),\n",
        "        \tlayers.Dropout(rate=0.5),\n",
        "        \tlayers.ReLU(),\n",
        "        \tlayers.Dense(1)])\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        x = inputs # [b, 80]\n",
        "        # embedding: [b, 80] => [b, 80, 100]\n",
        "        x = self.embedding(x)\n",
        "        # rnn cell compute,[b, 80, 100] => [b, 64]\n",
        "        x = self.rnn(x)\n",
        "        # 末层最后一个输出作为分类网络的输入: [b, 64] => [b, 1]\n",
        "        x = self.outlayer(x,training)\n",
        "        # p(y is pos|x)\n",
        "        prob = tf.sigmoid(x)\n",
        "\n",
        "        return prob\n",
        "\n",
        "def main():\n",
        "    units = 32 # RNN状态向量长度f\n",
        "    epochs = 1 # 训练epochs\n",
        "\n",
        "    model = MyRNN(units)\n",
        "    # 装配\n",
        "    model.compile(optimizer = optimizers.Adam(0.001),\n",
        "                  loss = losses.BinaryCrossentropy(),\n",
        "                  metrics=['accuracy'])\n",
        "    # 训练和验证\n",
        "    model.fit(db_train, epochs=epochs, validation_data=db_test)\n",
        "    # 测试\n",
        "    model.evaluate(db_test)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "2.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b63af05273f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#uploaded = files.upload()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tran.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cp1252'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAaE8M81NaVS",
        "colab_type": "text"
      },
      "source": [
        "# 新しいセクション"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FtVss6oNjXk",
        "colab_type": "code",
        "outputId": "3c25690e-95a6-4fa5-bd4e-f7071c5a6aee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "df = pd.read_csv('tran.csv',encoding='cp1252')\n",
        "#df['date'] = pd.Categorical(df['date'])\n",
        "#df['date'] = df.date.cat.codes\n",
        "a=df.loc[:,['date','start','high','low','end','trading_volume']]\n",
        "b=df.loc[:,['trading_volume']]\n",
        "#np.arrayに変換\n",
        "npdata=np.array(a.values)\n",
        "npdata[:10,:]\n",
        "\n",
        "#nparray=np.reshape(npdata,(data.shape[0],data.shape[1]))\n",
        "#for idx in range(5,400):\n",
        "#  np.append([npdata[idx-4:idx]])\n",
        "\n",
        "#target = df.pop('result')\n",
        "#dataset = tf.data.Dataset.from_tensor_slices((df.values, target.values))\n",
        "#for feat, targ in dataset.take(20):\n",
        "##  print ('Features: {}, Target: {}'.format(feat, targ))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  43104,     149,     154,     149,     152, 1832900],\n",
              "       [  43105,     152,     158,     150,     157, 2952500],\n",
              "       [  43109,     158,     159,     156,     157, 2694500],\n",
              "       [  43110,     157,     166,     157,     164, 7053300],\n",
              "       [  43111,     165,     168,     162,     167, 6238800],\n",
              "       [  43112,     168,     168,     161,     162, 5936700],\n",
              "       [  43115,     162,     162,     157,     162, 4325100],\n",
              "       [  43116,     161,     162,     158,     158, 2084400],\n",
              "       [  43117,     158,     160,     154,     154, 2761500],\n",
              "       [  43118,     156,     161,     155,     158, 2615000]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    }
  ]
}